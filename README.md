# LLM-Papers

2017

Attention Is All You Need -> https://lnkd.in/gF2X6HbE
Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer -> https://lnkd.in/gRDQpkzw
Proximal Policy Optimization Algorithms -> https://lnkd.in/g7NX8mPr
Deep Reinforcement Learning from Human Preferences -> https://lnkd.in/gDrDVMHG

2018

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding -> https://lnkd.in/gfVWKUjJ
Deep contextualized word representations (ELMo) -> https://lnkd.in/gK3szKDM
Improving Language Understanding by Generative Pre-Training (GPT) -> https://lnkd.in/gbcJETKy
SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing -> https://lnkd.in/g7B_xu9g
Universal Language Model Fine-tuning for Text Classification (ULMFiT) -> https://lnkd.in/guiMi8DE

2019

Language Models are Unsupervised Multitask Learners (GPT-2) -> https://lnkd.in/gYq2tVvr
RoBERTa: A Robustly Optimized BERT Pretraining Approach -> https://lnkd.in/gsvSJmh7
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter -> https://lnkd.in/gF5Z4A2y
BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension -> https://lnkd.in/ggqZdfmZ
XLNet: Generalized Autoregressive Pretraining for Language Understanding -> https://lnkd.in/gK4ENGFU
Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism -> https://lnkd.in/gfEnnf7Y
Generating Long Sequences with Sparse Transformers -> https://lnkd.in/gjbsWemR

2020

Reformer: The Efficient Transformer -> https://lnkd.in/gz8_Ftkq
Longformer: The Long-Document Transformer -> https://lnkd.in/gNBJ9Qtc
GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding -> https://lnkd.in/gJTFfgcm
Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks -> https://lnkd.in/gyS3BX7q
Big Bird: Transformers for Longer Sequences -> https://lnkd.in/gYZuKxKk
GPT-3 -> https://lnkd.in/gFVBFQap
Rethinking Attention with Performers -> https://lnkd.in/gTtdQw7w
T5 -> https://lnkd.in/gSNZnntD
Measuring Massive Multitask Language Understanding -> https://lnkd.in/geGYGgRx
ZeRO (Zero Redundancy Optimizer) -> https://lnkd.in/gp-bTGmM
ELECTRA -> https://lnkd.in/gC9QNxvA
Scaling Laws -> https://lnkd.in/gHvNumQd
