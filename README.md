#  LLM Papers by Year

---

## ðŸ“˜ 2017

- [Attention Is All You Need](https://lnkd.in/gF2X6HbE)  
- [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://lnkd.in/gRDQpkzw)  
- [Proximal Policy Optimization Algorithms](https://lnkd.in/g7NX8mPr)  
- [Deep Reinforcement Learning from Human Preferences](https://arxiv.org/abs/1706.03741)

---

## ðŸ“˜ 2018

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)  
- [Deep contextualized word representations (ELMo)](https://arxiv.org/abs/1802.05365)  
- [Improving Language Understanding by Generative Pre-Training (GPT)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)  
- [SentencePiece: A simple and language independent subword tokenizer and detokenizer](https://arxiv.org/abs/1808.06226)  
- [Universal Language Model Fine-tuning for Text Classification (ULMFiT)](https://arxiv.org/abs/1801.06146)

---

## ðŸ“˜ 2019

- [Language Models are Unsupervised Multitask Learners (GPT-2)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
- [RoBERTa](https://arxiv.org/abs/1907.11692)  
- [DistilBERT](https://arxiv.org/abs/1910.01108)  
- [BART](https://arxiv.org/abs/1910.13461)  
- [XLNet](https://arxiv.org/abs/1906.08237)  
- [Megatron-LM](https://arxiv.org/abs/1909.08053)  
- [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)

---

## ðŸ“˜ 2020

- [Reformer](https://arxiv.org/abs/2001.04451)  
- [Longformer](https://arxiv.org/abs/2004.05150)  
- [GShard](https://arxiv.org/abs/2006.16668)  
- [Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401)  
- [Big Bird](https://arxiv.org/abs/2007.14062)  
- [GPT-3](https://arxiv.org/abs/2005.14165)  
- [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794v4)  
- [T5](https://arxiv.org/abs/1910.10683)  
- [Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300)  
- [ZeRO (Zero Redundancy Optimizer)](https://arxiv.org/abs/1910.02054)  
- [ELECTRA](https://arxiv.org/abs/2003.10555)  
- [Scaling Laws](https://arxiv.org/abs/2001.08361)

---

## ðŸ“˜ 2021

- [RoFormer](https://arxiv.org/abs/2104.09864)  
- [Megatron-LM (GPU Cluster Training)](https://arxiv.org/abs/2104.04473)  
- [Transcending Scaling Laws with 0.1% Extra Compute](https://arxiv.org/abs/2210.11399)  
- [Retrieval-Augmented LMs from Trillions of Tokens](https://arxiv.org/abs/2112.04426)  
- [LoRA](https://arxiv.org/abs/2106.09685)  
- [Gopher](https://arxiv.org/abs/2112.11446)  
- [Megatron-Turing NLG](https://arxiv.org/abs/2201.11990)

---

## ðŸ“˜ 2022

- [Efficient Transformer Inference](https://arxiv.org/pdf/2211.05102)  
- [Speculative Decoding](https://arxiv.org/abs/2211.17192)  
- [Chinchilla](https://arxiv.org/abs/2203.15556)  
- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)  
- [InstructGPT](https://arxiv.org/abs/2203.02155)  
- [BLOOM](https://arxiv.org/abs/2211.05100)  
- [Emergent Abilities](https://arxiv.org/abs/2206.07682)  
- [Flash Attention](https://arxiv.org/abs/2205.14135)  
- [Grouped-query Attention](https://arxiv.org/abs/2305.13245)  
- [ALiBi](https://arxiv.org/abs/2108.12409)  
- [DeepSpeed Inference](https://arxiv.org/abs/2207.00032)  
- [Red Teaming LMs](https://arxiv.org/abs/2202.03286)  
- [HELM](https://arxiv.org/abs/2211.09110)  
- [GPTQ](https://arxiv.org/abs/2210.17323)  
- [Beyond the Imitation Game](https://arxiv.org/abs/2206.04615)

---

## ðŸ“˜ 2023

- [LLaMA](https://arxiv.org/abs/2302.13971)  
- [LLaMA 2](https://arxiv.org/abs/2307.09288)  
- [PagedAttention](https://arxiv.org/abs/2309.06180)  
- [QLoRA](https://arxiv.org/abs/2305.14314)  
- [Parameter-Efficient Fine-Tuning Review](https://arxiv.org/abs/2312.12148)  
- [FlashAttention-2](https://arxiv.org/abs/2307.08691)  
- [AWQ](https://arxiv.org/abs/2306.00978)  
- [Generative Agents](https://arxiv.org/abs/2304.03442)  
- [Voyager](https://arxiv.org/abs/2305.16291)  
- [Universal Adversarial Attacks](https://arxiv.org/abs/2307.15043)  
- [Tree of Thoughts](https://arxiv.org/abs/2305.10601)  
- [WizardLM](https://arxiv.org/abs/2304.12244)  
- [DeepSpeed-Chat](https://arxiv.org/abs/2308.01320)  
- [GPT-4](https://arxiv.org/abs/2303.08774)  
- [Mistral 7B](https://arxiv.org/abs/2310.06825)  
- [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290)  
- [PaLM 2](https://arxiv.org/abs/2305.10403)  
- [LIMA](https://arxiv.org/abs/2305.11206)  
- [Mamba](https://arxiv.org/abs/2312.00752)  
- [LLaVA](https://arxiv.org/abs/2304.08485)  
- [phi-1.5](https://arxiv.org/abs/2309.05463)  
- [Qwen Technical Report](https://arxiv.org/abs/2309.16609)  
- [Qwen-VL](https://arxiv.org/abs/2308.12966)

---

## ðŸ“˜ 2024

- [Chatbot Arena](https://arxiv.org/abs/2403.04132)  
- [TinyLlama](https://arxiv.org/abs/2401.02385)  
- [Jamba](https://arxiv.org/abs/2403.19887)  
- [Qwen2-VL](https://arxiv.org/abs/2409.12191)  
- [LLaMA 3](https://arxiv.org/abs/2407.21783)  
- [FlashAttention-3](https://arxiv.org/abs/2407.08608)

---

## ðŸ“˜ 2025

- [DeepSeek-R1: Incentivizing Reasoning via RL](https://arxiv.org/abs/2501.12948)
